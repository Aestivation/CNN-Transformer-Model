{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOu7RfVMp/paGHaRL9BsEQw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aestivation/CNN-Transformer-Model/blob/KITTY-Dataset/CNN%2BTransformer_Hybrid_Model_KITTI_DATASET_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Connect Google Colab to Google Drive (if using Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download KITTI dataset from Kaggle\n",
        "!kaggle datasets download -d klemenko/kitti-dataset\n",
        "!unzip -q kitti-dataset.zip -d /content/kitti_data/\n",
        "\n",
        "# Define dataset paths\n",
        "IMG_DIR = \"/content/kitti_data/data_object_image_2/training/image_2/\"\n",
        "LABEL_DIR = \"/content/kitti_data/data_object_label_2/training/label_2/\"\n",
        "\n",
        "# Check the number of images in the dataset\n",
        "image_files = os.listdir(IMG_DIR)\n",
        "print(\"Total images in KITTI dataset:\", len(image_files))\n",
        "\n",
        "# Display a sample image from the dataset\n",
        "sample_img = load_img(os.path.join(IMG_DIR, image_files[0]))\n",
        "plt.imshow(sample_img)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Sample KITTI Image\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "s2R_X-d99KCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Image size for training\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "# Function to load and preprocess images and labels\n",
        "def load_kitti_data(img_dir, label_dir, img_size=IMG_SIZE, max_samples=1000):\n",
        "    images, labels = [], []\n",
        "    image_files = os.listdir(img_dir)[:max_samples]  # Load only a subset\n",
        "\n",
        "    for img_file in image_files:\n",
        "        # Load image and normalize\n",
        "        img_path = os.path.join(img_dir, img_file)\n",
        "        img = load_img(img_path, target_size=img_size)\n",
        "        img = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "        images.append(img)\n",
        "\n",
        "        # Load corresponding label file\n",
        "        label_file = img_file.replace(\".png\", \".txt\")\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                if len(lines) > 0:\n",
        "                    parts = lines[0].strip().split()\n",
        "                    x1, y1, x2, y2 = map(float, parts[4:8])\n",
        "\n",
        "                    # Normalize bounding box coordinates\n",
        "                    labels.append([x1 / img_size[0], y1 / img_size[1], x2 / img_size[0], y2 / img_size[1]])\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load dataset\n",
        "X_train, y_train = load_kitti_data(IMG_DIR, LABEL_DIR)\n",
        "\n",
        "# Print dataset shape\n",
        "print(f\"Images shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
        "\n",
        "# Show a sample image with its bounding box\n",
        "def show_sample_image(img, bbox):\n",
        "    img = (img * 255).astype(np.uint8)  # Convert back to 0-255 range\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    x1, y1, x2, y2 = int(x1 * IMG_SIZE[0]), int(y1 * IMG_SIZE[1]), int(x2 * IMG_SIZE[0]), int(y2 * IMG_SIZE[1])\n",
        "\n",
        "# This code creates a copy of the image and then uses cv2.rectangle() to draw a blue bounding box at coordinates (x1, y1, x2, y2) to visualize the detected object:\n",
        "    img_with_box = img.copy()\n",
        "    img_with_box = cv2.rectangle(img_with_box, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "    plt.imshow(img_with_box)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Display a sample image with bounding box\n",
        "show_sample_image(X_train[0], y_train[0])\n"
      ],
      "metadata": {
        "id": "ncXgkshAAFv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Define the CNN Backbone (ResNet50)\n",
        "def build_cnn_feature_extractor(input_shape=(224, 224, 3)):\n",
        "    # Load ResNet50 without the top classification layer (head)\n",
        "    base_cnn = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Freeze pre-trained weights to prevent modification during training\n",
        "    base_cnn.trainable = False\n",
        "\n",
        "    # Preserve spatial structure: Reshape CNN output from (7, 7, 2048) to (49, 2048)\n",
        "    cnn_output = Reshape((49, 2048))(base_cnn.output)  # âœ… Fix applied\n",
        "\n",
        "    # Define the final feature extractor model\n",
        "    cnn_model = Model(inputs=base_cnn.input, outputs=cnn_output)\n",
        "\n",
        "    return cnn_model\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = build_cnn_feature_extractor()\n",
        "\n",
        "# Print CNN model summary\n",
        "cnn_model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "UutMl2b4LEvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "\n",
        "# Define Transformer input dimensions\n",
        "D_MODEL = 512  # Embedding dimension per patch\n",
        "NUM_PATCHES = 49  # Matches the CNN output patches (7x7 -> 49 patches)\n",
        "\n",
        "# Define a custom layer to reshape CNN features for Transformer input\n",
        "class ReshapeCNNFeatures(Layer):\n",
        "    def __init__(self, embed_dim, num_patches):\n",
        "        super(ReshapeCNNFeatures, self).__init__()\n",
        "        self.dense = Dense(embed_dim)  # Reduce feature size to match Transformer D_MODEL\n",
        "        self.num_patches = num_patches\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        print(f\"Shape before Dense layer: {inputs.shape}\")  # Debugging print\n",
        "        x = self.dense(inputs)  # Convert CNN features to embedding space\n",
        "        print(f\"Shape after Dense layer: {x.shape}\")  # Debugging print\n",
        "\n",
        "        reshaped_x = tf.reshape(x, (-1, self.num_patches, self.embed_dim))  # Reshape for Transformer\n",
        "        print(f\"Shape after Reshaping: {reshaped_x.shape}\")  # Debugging print\n",
        "        return reshaped_x\n",
        "\n",
        "# Initialize the reshape layer\n",
        "reshape_layer = ReshapeCNNFeatures(D_MODEL, NUM_PATCHES)\n"
      ],
      "metadata": {
        "id": "iG8DOTdJYOJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Dense\n",
        "\n",
        "# Define Transformer Encoder Block\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),  # Expanding feature space\n",
        "            Dense(embed_dim)  # Compressing back to original dimensions\n",
        "        ])\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # Apply self-attention mechanism\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(inputs + attn_output)  # Residual connection & normalization\n",
        "\n",
        "        # Pass through feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        print(f\"Transformer Output Sample: {ffn_output.numpy()[0, :5]}\")\n",
        "        return self.norm2(out1 + ffn_output)  # Final residual connection & normalization\n"
      ],
      "metadata": {
        "id": "soRwQjRLOtVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dropout\n",
        "\n",
        "# Define the Hybrid CNN + Transformer Model\n",
        "def build_cnn_transformer_model():\n",
        "    inputs = Input(shape=(224, 224, 3))  # Input image\n",
        "\n",
        "    # CNN Feature Extraction\n",
        "    cnn_features = cnn_model(inputs)\n",
        "\n",
        "    # Reshape CNN features for Transformer input\n",
        "    cnn_features = reshape_layer(cnn_features)\n",
        "\n",
        "    # Apply Transformer Encoder\n",
        "   transformer_block = TransformerBlock(embed_dim=D_MODEL, num_heads=4, ff_dim=1024, rate=0.2)\n",
        "   transformer_output = transformer_block(cnn_features, training=True)  # Pass training argument\n",
        "\n",
        "    # Flatten the Transformer output\n",
        "    x = Flatten()(transformer_output)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Output layer predicts bounding box (x1, y1, x2, y2)\n",
        "    outputs = Dense(4, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Build the model\n",
        "model = build_cnn_transformer_model()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "8gmnrZTXYmox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),  # Adaptive learning rate optimizer\n",
        "    loss=Huber(delta=1.0),  # Loss function for bounding box regression\n",
        "    metrics=[MeanAbsoluteError()]  # Track error in bounding box prediction\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,  # Number of training iterations\n",
        "    batch_size=32,  # Number of samples per batch\n",
        "    validation_split=0.1  # Use 10% of data for validation\n",
        ")\n"
      ],
      "metadata": {
        "id": "W_yeTmoOY4bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to draw bounding boxes on images\n",
        "def draw_bounding_box(image, bbox, color=(0, 255, 0)):\n",
        "    \"\"\" Draws a bounding box on the image using OpenCV \"\"\"\n",
        "    img = (image * 255).astype(np.uint8)  # Convert back to 0-255 range\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    x1, y1, x2, y2 = int(x1 * IMG_SIZE[0]), int(y1 * IMG_SIZE[1]), int(x2 * IMG_SIZE[0]), int(y2 * IMG_SIZE[1])\n",
        "\n",
        "    img_with_box = img.copy()\n",
        "    img_with_box = cv2.rectangle(img_with_box, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "    return img_with_box\n",
        "\n",
        "# Select a few test samples\n",
        "num_samples = 5\n",
        "sample_images = X_train[:num_samples]\n",
        "true_bboxes = y_train[:num_samples]\n",
        "\n",
        "# Predict bounding boxes for test images\n",
        "predicted_bboxes = model.predict(sample_images)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(1, num_samples, i + 1)\n",
        "    img_with_pred = draw_bounding_box(sample_images[i], predicted_bboxes[i])\n",
        "    plt.imshow(img_with_pred)\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SxUdxMlyYaC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}