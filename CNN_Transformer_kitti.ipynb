{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aestivation/CNN-Transformer-Model/blob/main/cnn_transformer_kitti.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQeACKHLNBpH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create the .kaggle directory if it does not exist\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Move the kaggle.json file to the correct location\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set proper permissions to avoid permission errors\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK9QnXAhNBfx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download KITTI dataset from Kaggle\n",
        "!kaggle datasets download -d klemenko/kitti-dataset\n",
        "!unzip -q kitti-dataset.zip -d /content/kitti_data/\n",
        "\n",
        "# Define dataset paths\n",
        "IMG_DIR = \"/content/kitti_data/data_object_image_2/training/image_2/\"\n",
        "LABEL_DIR = \"/content/kitti_data/data_object_label_2/training/label_2/\"\n",
        "\n",
        "# Number of images for training\n",
        "NUM_SAMPLES = 5000\n",
        "\n",
        "# Data Augmentation to enhance the dataset\n",
        "data_gen = ImageDataGenerator(\n",
        "    rotation_range=10,       # Random rotation up to 10 degrees\n",
        "    width_shift_range=0.1,   # Random horizontal shift up to 10% of width\n",
        "    height_shift_range=0.1,  # Random vertical shift up to 10% of height\n",
        "    horizontal_flip=True     # Randomly flip images horizontally\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFnNkKTHOSer"
      },
      "outputs": [],
      "source": [
        "def load_kitti_data(img_dir, label_dir, num_samples=NUM_SAMPLES):\n",
        "    images, labels = [], []\n",
        "    img_size = (224, 224)\n",
        "\n",
        "    for i, img_file in enumerate(sorted(os.listdir(img_dir))):\n",
        "        if i >= num_samples:\n",
        "            break  # Limit the number of samples to reduce memory usage\n",
        "\n",
        "        img_path = os.path.join(img_dir, img_file)\n",
        "        img = cv2.imread(img_path)\n",
        "        original_shape = img.shape[:2]  # (height, width)\n",
        "        img = cv2.resize(img, img_size) / 255.0  # Normalize pixel values\n",
        "        images.append(img)\n",
        "\n",
        "        # Load corresponding label file\n",
        "        label_file = img_file.replace('.png', '.txt')\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                if len(lines) > 0:\n",
        "                    parts = lines[0].strip().split()\n",
        "                    x1, y1, x2, y2 = map(float, parts[4:8])\n",
        "\n",
        "                    # Normalize bounding box coordinates based on original image size\n",
        "                    x1 /= original_shape[1]\n",
        "                    x2 /= original_shape[1]\n",
        "                    y1 /= original_shape[0]\n",
        "                    y2 /= original_shape[0]\n",
        "\n",
        "                    labels.append([x1, y1, x2, y2])\n",
        "                else:\n",
        "                    labels.append([0, 0, 1, 1])  # Default bounding box if no label exists\n",
        "        else:\n",
        "            labels.append([0, 0, 1, 1])  # Default bounding box if label file is missing\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load training data\n",
        "X_train, y_train = load_kitti_data(IMG_DIR, LABEL_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VEtRchdROVaU"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model():\n",
        "    inputs = keras.Input(shape=(224, 224, 3))\n",
        "    base_model = keras.applications.VGG16(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
        "    base_model.trainable = False\n",
        "    cnn_features = base_model.output\n",
        "\n",
        "    cnn_features = layers.GlobalAveragePooling2D()(cnn_features)\n",
        "    cnn_features = layers.Reshape((1, cnn_features.shape[-1]))(cnn_features)\n",
        "\n",
        "    return keras.Model(inputs, cnn_features, name=\"CNN_Feature_Extractor\")\n",
        "\n",
        "cnn_model = build_cnn_model()\n",
        "cnn_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MK83A5-3O5VE"
      },
      "outputs": [],
      "source": [
        "def build_transformer_block(embed_dim=512, num_heads=4, ff_dim=1024, dropout_rate=0.3):\n",
        "    inputs = keras.Input(shape=(1, embed_dim))\n",
        "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
        "    attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "\n",
        "    ffn = keras.Sequential([\n",
        "        layers.Dense(ff_dim, activation=\"relu\"),\n",
        "        layers.Dense(embed_dim),\n",
        "    ])\n",
        "\n",
        "    ffn_output = ffn(out1)\n",
        "    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "    outputs = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=\"Transformer_Block\")\n",
        "\n",
        "transformer_block = build_transformer_block()\n",
        "transformer_block.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-4dJRvCAO5Rq"
      },
      "outputs": [],
      "source": [
        "def build_cnn_transformer_model():\n",
        "    inputs = keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "    # Extract features using CNN\n",
        "    cnn_features = cnn_model(inputs)\n",
        "\n",
        "    # Process features with Transformer\n",
        "    transformer_output = transformer_block(cnn_features)\n",
        "\n",
        "    # Flatten the output for Bounding Box prediction\n",
        "    x = layers.Flatten()(transformer_output)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Final output with 4 values (Bounding Box: x1, y1, x2, y2)\n",
        "    outputs = layers.Dense(4, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=\"CNN_Transformer_Detector\")\n",
        "\n",
        "# Build the model\n",
        "model = build_cnn_transformer_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SUc0wtbO5O1"
      },
      "outputs": [],
      "source": [
        "# Model Compilation Settings\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=\"mse\",\n",
        "    metrics=[\"mae\"]\n",
        ")\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define Early Stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",  # Monitor validation loss\n",
        "    patience=5,          # Stop training if no improvement for 5 epochs\n",
        "    restore_best_weights=True,  # Restore the best weights after stopping\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the best model based on validation loss\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Model Training\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=40,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping, checkpoint]  # Include early stopping and checkpointing\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urg6hbotO5Mv"
      },
      "outputs": [],
      "source": [
        "# Plot Training Loss and Validation Loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training & Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6Crk9OsQ76h"
      },
      "outputs": [],
      "source": [
        "# Select the last 20 training samples as test data\n",
        "X_test = X_train[-20:]\n",
        "y_test = y_train[-20:]\n",
        "\n",
        "# Predict Bounding Box coordinates using the model\n",
        "predicted_bboxes = model.predict(X_test)\n",
        "\n",
        "# Display some prediction samples\n",
        "for i in range(5):\n",
        "    img = X_test[i] * 255  # Restore original scale of the image\n",
        "    bbox = predicted_bboxes[i]\n",
        "\n",
        "    # Convert Bounding Box from normalized scale to pixel values\n",
        "    h, w = img.shape[:2]\n",
        "    x1, y1, x2, y2 = int(bbox[0] * w), int(bbox[1] * h), int(bbox[2] * w), int(bbox[3] * h)\n",
        "\n",
        "    # Display image with predicted Bounding Box\n",
        "    plt.imshow(img.astype(np.uint8))\n",
        "    plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none'))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xYbku0pR3eh"
      },
      "outputs": [],
      "source": [
        "# Save the trained model for future use\n",
        "model.save(\"cnn_transformer_kitti_trained.h5\")\n",
        "print(\"Model was saved.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
